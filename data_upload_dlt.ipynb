{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.rest_api import rest_api_source\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dlt.resource(name=\"courses\")   # The name of the resource (used as the table name)\n",
    "def fetch_courses_pipeline():\n",
    "    client = RESTClient(  # Initialize REST client without params\n",
    "        base_url=\"https://opendata.caissedesdepots.fr/api/explore/v2.1\",\n",
    "        paginator=PageNumberPaginator(\n",
    "            base_page=1,\n",
    "            total_path=None\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Iterate over paginated responses\n",
    "    for page in client.paginate(\"/catalog/datasets/moncompteformation_catalogueformation/exports/csv\"):\n",
    "        yield page\n",
    "\n",
    "# Define new dlt pipeline\n",
    "pipeline = dlt.pipeline(destination=\"filesystem\")\n",
    "\n",
    "# Run the pipeline with the new resource\n",
    "load_info = pipeline.run(fetch_courses_pipeline, write_disposition=\"replace\")\n",
    "print(load_info)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.rest_api import rest_api_source\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "from dlt.sources.filesystem import filesystem\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from dlt.sources.rest_api import rest_api_source\n",
    "from dlt.sources.helpers.rest_client import RESTClient\n",
    "from dlt.sources.helpers.rest_client.paginators import PageNumberPaginator\n",
    "from dlt.sources.filesystem import filesystem\n",
    "import pandas as pd\n",
    "\n",
    "url=\"https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/moncompteformation_catalogueformation/exports/csv\"\n",
    "\n",
    "@dlt.resource(name=\"courses\")   # The name of the resource (used as the table name)\n",
    "def fetch_courses_pipeline():\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            buffer = io.BytesIO()\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):  # 1MB chunks\n",
    "                buffer.write(chunk)\n",
    "            buffer.seek(0)\n",
    "            table = pd.read_csv(buffer)\n",
    "            print(f'Got data from {url} with {table.num_rows} records')\n",
    "            if table.num_rows > 0:\n",
    "                yield table\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data from {url}: {e}\")\n",
    "\n",
    "# Define new dlt pipeline\n",
    "pipeline = dlt.pipeline(destination=\"filesystem\")\n",
    "\n",
    "# Run the pipeline with the new resource\n",
    "load_info = pipeline.run(fetch_courses_pipeline(), write_disposition=\"replace\")\n",
    "print(load_info)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "# Define the base URL and refinement parameters\n",
    "BASE_URL = \"https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/moncompteformation_catalogueformation/exports/csv\"\n",
    "params = {\n",
    "    \"refine\": [\n",
    "        \"libelle_nsf_1:Informatique, traitement de l'information, réseaux de transmission\",\n",
    "        \"libelle_nsf_1:Enseignement, formation\",\n",
    "        \"libelle_nsf_1:Commerce, vente\",\n",
    "        \"libelle_nsf_1:Comptabilite, gestion\",\n",
    "        \"libelle_nsf_1:Spécialités pluri-scientifiques\",\n",
    "        \"libelle_nsf_1:Spécialites plurivalentes de la communication et de l'information\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Set the GCS credentials environment variable (make sure to provide the correct JSON key path)\n",
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./gcs.json\"\n",
    "\n",
    "# Define the GCS bucket and prefix for storage\n",
    "GCS_BUCKET_NAME = \"jugnu-france-course-enrollments\"\n",
    "GCS_PREFIX = \"french_courses/\"\n",
    "\n",
    "@dlt.resource(name=\"french_courses\")\n",
    "def fetch_courses():\n",
    "    \"\"\"Fetches and yields data from the API in chunks.\"\"\"\n",
    "    try:\n",
    "        # Send the GET request to the API with parameters\n",
    "        response = requests.get(BASE_URL, params=params, stream=True)\n",
    "        response.raise_for_status()  # Raise an error for failed requests\n",
    "\n",
    "        # Create a buffer to read the CSV data\n",
    "        buffer = io.StringIO(response.text)\n",
    "        df = pd.read_csv(buffer)\n",
    "\n",
    "        # If the response contains data, yield the dataframe\n",
    "        if not df.empty:\n",
    "            print(f\"Fetched {len(df)} records\")\n",
    "            yield df\n",
    "        else:\n",
    "            print(\"No data found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data: {e}\")\n",
    "\n",
    "# Define a function to upload the data to GCS\n",
    "def upload_to_gcs(df, file_name):\n",
    "    \"\"\"Uploads the dataframe to GCS as a CSV file.\"\"\"\n",
    "    try:\n",
    "        # Convert DataFrame to CSV and upload to GCS\n",
    "        csv_buffer = io.StringIO()\n",
    "        df.to_csv(csv_buffer, index=False)\n",
    "        csv_buffer.seek(0)\n",
    "\n",
    "        # Initialize the GCS client\n",
    "        client = storage.Client()\n",
    "\n",
    "        # Set the GCS blob (file) path\n",
    "        gcs_blob_path = GCS_PREFIX + file_name\n",
    "        bucket = client.get_bucket(GCS_BUCKET_NAME)\n",
    "        blob = bucket.blob(gcs_blob_path)\n",
    "\n",
    "        # Upload CSV content to the GCS bucket\n",
    "        blob.upload_from_file(csv_buffer, content_type=\"text/csv\")\n",
    "        print(f\"File uploaded to GCS: {gcs_blob_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload to GCS: {e}\")\n",
    "\n",
    "# Configure the pipeline to send data to GCS (no BigQuery here)\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"french_courses_pipeline\",\n",
    "    destination=\"filesystem\",  # Set destination as GCS\n",
    "    dataset_name=\"french_courses_dataset\",\n",
    "    dev_mode=True\n",
    ")\n",
    "\n",
    "# Run the pipeline and load data\n",
    "load_info = pipeline.run(fetch_courses())\n",
    "\n",
    "# Once data is processed, upload to GCS\n",
    "for idx, df in enumerate(load_info):\n",
    "    file_name = f\"french_courses_part_{idx + 1}.csv\"\n",
    "    upload_to_gcs(df, file_name)\n",
    "\n",
    "print(\"Data upload to GCS completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/moncompteformation_catalogueformation/exports/csv\"\n",
    "\n",
    "@dlt.resource(name=\"courses\")\n",
    "def fetch_courses_pipeline():\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            buffer = io.BytesIO()\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                buffer.write(chunk)\n",
    "            buffer.seek(0)\n",
    "            table = pd.read_csv(buffer, sep=\";\") #Added sep parameter\n",
    "            print(f'Got data from {url} with {len(table)} records') #Used len(table)\n",
    "            if len(table) > 0:\n",
    "                yield table\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data from {url}: {e}\")\n",
    "\n",
    "# Define new dlt pipeline\n",
    "pipeline = dlt.pipeline(destination=\"filesystem\")\n",
    "\n",
    "# Run the pipeline with the new resource\n",
    "load_info = pipeline.run(fetch_courses_pipeline(), write_disposition=\"replace\")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/dbmm0pzn5wg9dfflt4yv3sd00000gn/T/ipykernel_1953/2617981311.py:17: DtypeWarning: Columns (44,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  table = pd.read_csv(buffer, sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data from https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/moncompteformation_catalogueformation/exports/csv with 201175 records\n",
      "Pipeline moncompteformation_pipeline load step completed in 13.91 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset courses_data\n",
      "The filesystem destination used gs://jugnu-france-course-enrollments location to store data\n",
      "Load package 1742298130.7551758 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/moncompteformation_catalogueformation/exports/csv\"\n",
    "\n",
    "@dlt.resource(name=\"courses\")\n",
    "def fetch_courses_pipeline():\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            buffer = io.BytesIO()\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                buffer.write(chunk)\n",
    "            buffer.seek(0)\n",
    "            table = pd.read_csv(buffer, sep=\";\")\n",
    "            print(f'Got data from {url} with {len(table)} records')\n",
    "            if len(table) > 0:\n",
    "                table['code_region'] = table['code_region'].astype(str)\n",
    "                table['coderegion_export'] = table['coderegion_export'].astype(str)\n",
    "                yield table\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data from {url}: {e}\")\n",
    "\n",
    "# Define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"moncompteformation_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"courses_data\"  # Top-level folder name\n",
    ")\n",
    "\n",
    "# Run the pipeline with the new resource, specify table name and destination path\n",
    "load_info = pipeline.run(\n",
    "    fetch_courses_pipeline(),\n",
    "    write_disposition=\"replace\",\n",
    "    table_name=\"courses_france\"\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "\n",
    "try:\n",
    "    pipeline = dlt.pipeline(destination=\"filesystem\")\n",
    "    print(\"GCS destination is available.\")\n",
    "except AttributeError as e:\n",
    "    print(f\"GCS destination is not available: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
