{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/td/dbmm0pzn5wg9dfflt4yv3sd00000gn/T/ipykernel_1953/2617981311.py:17: DtypeWarning: Columns (44,46) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  table = pd.read_csv(buffer, sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data from https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/moncompteformation_catalogueformation/exports/csv with 201175 records\n",
      "Pipeline moncompteformation_pipeline load step completed in 13.91 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset courses_data\n",
      "The filesystem destination used gs://jugnu-france-course-enrollments location to store data\n",
      "Load package 1742298130.7551758 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/moncompteformation_catalogueformation/exports/csv\"\n",
    "\n",
    "@dlt.resource(name=\"courses\")\n",
    "def fetch_courses_pipeline():\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            buffer = io.BytesIO()\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                buffer.write(chunk)\n",
    "            buffer.seek(0)\n",
    "            table = pd.read_csv(buffer, sep=\";\")\n",
    "            print(f'Got data from {url} with {len(table)} records')\n",
    "            if len(table) > 0:\n",
    "                table['code_region'] = table['code_region'].astype(str)\n",
    "                table['coderegion_export'] = table['coderegion_export'].astype(str)\n",
    "                yield table\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data from {url}: {e}\")\n",
    "\n",
    "# Define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"moncompteformation_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"courses_data\"  # Top-level folder name\n",
    ")\n",
    "\n",
    "# Run the pipeline with the new resource, specify table name and destination path\n",
    "load_info = pipeline.run(\n",
    "    fetch_courses_pipeline(),\n",
    "    write_disposition=\"replace\",\n",
    "    table_name=\"courses_france\"\n",
    ")\n",
    "print(load_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got data from https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/entree_sortie_formation/exports/csv with 181267 records\n",
      "Pipeline moncompteformation_pipeline load step completed in 2.72 seconds\n",
      "1 load package(s) were loaded to destination filesystem and into dataset enrollments_data\n",
      "The filesystem destination used gs://jugnu-france-course-enrollments location to store data\n",
      "Load package 1742298821.773914 is LOADED and contains no failed jobs\n"
     ]
    }
   ],
   "source": [
    "import dlt\n",
    "import requests\n",
    "import io\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "url = \"https://opendata.caissedesdepots.fr/api/explore/v2.1/catalog/datasets/entree_sortie_formation/exports/csv\"\n",
    "\n",
    "@dlt.resource(name=\"enrollments\")\n",
    "def fetch_courses_pipeline():\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as response:\n",
    "            response.raise_for_status()\n",
    "            buffer = io.BytesIO()\n",
    "            for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                buffer.write(chunk)\n",
    "            buffer.seek(0)\n",
    "            table = pd.read_csv(buffer, sep=\";\")\n",
    "            print(f'Got data from {url} with {len(table)} records')\n",
    "            if len(table) > 0:\n",
    "                yield table\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data from {url}: {e}\")\n",
    "\n",
    "# Define new dlt pipeline\n",
    "pipeline = dlt.pipeline(\n",
    "    pipeline_name=\"moncompteformation_pipeline\",\n",
    "    destination=\"filesystem\",\n",
    "    dataset_name=\"enrollments_data\"  # Top-level folder name\n",
    ")\n",
    "\n",
    "# Run the pipeline with the new resource, specify table name and destination path\n",
    "load_info = pipeline.run(\n",
    "    fetch_courses_pipeline(),\n",
    "    write_disposition=\"replace\",\n",
    "    table_name=\"enrollments_raw_parquet\"\n",
    ")\n",
    "print(load_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de-zoomcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
